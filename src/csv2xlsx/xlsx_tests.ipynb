{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "import pandas\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys: \n",
      " \tStatus, Task type, Topology name, Dataset, Framework, \n",
      " \tInference Framework, Input blob sizes, Precision, Batch size, Mode, \n",
      " \tParameters, Infrastructure, Average time of single pass (s), Latency, FPS\n"
     ]
    }
   ],
   "source": [
    "csv_file_name = 'results_total.csv'\n",
    "\n",
    "def read_csv_results(csv_file_name):\n",
    "    data = pandas.read_csv(csv_file_name, sep = ';', encoding='latin-1')\n",
    "    data_dictionary = data.to_dict()\n",
    "    return data, data_dictionary\n",
    "\n",
    "data, data_dictionary = read_csv_results(csv_file_name)\n",
    "\n",
    "keys = list(data_dictionary.keys())\n",
    "\n",
    "key_status = keys[0]\n",
    "key_task_type = keys[1]\n",
    "key_topology_name = keys[2]\n",
    "key_dataset = keys[3]\n",
    "key_train_framework = keys[4]\n",
    "key_inference_framework = keys[5]\n",
    "key_blob_size = keys[6]\n",
    "key_precision = keys[7]\n",
    "key_batch_size = keys[8]\n",
    "key_execution_mode = keys[9]\n",
    "key_parameters = keys[10]\n",
    "key_infrastructure = keys[11]\n",
    "key_avgtime = keys[12]\n",
    "key_latency = keys[13]\n",
    "key_fps = keys[14]\n",
    "\n",
    "print('Available keys: \\n \\\n",
    "\\t{0}, {1}, {2}, {3}, {4}, \\n \\\n",
    "\\t{5}, {6}, {7}, {8}, {9}, \\n \\\n",
    "\\t{10}, {11}, {12}, {13}, {14}'.format(key_status, key_task_type, key_topology_name, \\\n",
    "                                 key_dataset, key_train_framework, \\\n",
    "                                 key_inference_framework, key_blob_size, \\\n",
    "                                 key_precision, key_batch_size, \\\n",
    "                                 key_execution_mode, key_parameters, key_infrastructure,\\\n",
    "                                 key_avgtime, key_latency, key_fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_name = 'test.xlsx'\n",
    "excel_sheet_name = 'Performance'\n",
    "\n",
    "book = xlsxwriter.Workbook(excel_file_name)\n",
    "sheet = book.add_worksheet(excel_sheet_name)\n",
    "\n",
    "cell_format = book.add_format({'align': 'center', 'valign': 'vcenter', \\\n",
    "                               'border': 1, 'bold': True, 'text_wrap': True})\n",
    "sheet.merge_range('A1:A5', key_task_type, cell_format)\n",
    "sheet.merge_range('B1:B5', key_topology_name, cell_format)\n",
    "sheet.merge_range('C1:C5', key_train_framework, cell_format)\n",
    "sheet.merge_range('D1:D5', key_blob_size, cell_format)\n",
    "sheet.merge_range('E1:E5', key_batch_size, cell_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CPU: Intel(R) Core(TM) i3-8100 CPU @ 3.60GHz, CPU family: x86_64, GPU: Intel(R) Gen9 HD Graphics (iGPU), RAM size: 32757700 kB, OS family: Linux, OS version: Linux-5.4.0-73-generic-x86_64-with-Ubuntu-18.04-bionic, Python version: 3.6.9', 'CPU: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz, CPU family: x86_64, GPU: Intel(R) Gen9 HD Graphics (iGPU), RAM size: 65727428 kB, OS family: Linux, OS version: Linux-5.4.0-52-generic-x86_64-with-Ubuntu-18.04-bionic, Python version: 3.6.9', 'CPU: Intel(R) Xeon(R) Gold 6138 CPU @ 2.00GHz CPU family: x86_64, GPU: None, RAM size: 526980036 kB, OS family: Linux OS version: Linux-5.8.0-53-generic-x86_64-with-Ubuntu-18.04-bionic, Python version: 3.6.9']\n",
      "[['OpenVINO DLDT'], ['OpenVINO DLDT'], ['OpenVINO DLDT']]\n",
      "[[['CPU', 'GPU', 'MYRIAD']], [['CPU', 'GPU', 'MYRIAD']], [['CPU']]]\n"
     ]
    }
   ],
   "source": [
    "# get unique infrastructure\n",
    "infrastructure = list(set(list(data_dictionary[key_infrastructure].values())))\n",
    "\n",
    "# get inference frameworks for each infrastructure\n",
    "inference_frameworks = []\n",
    "for machine in infrastructure:\n",
    "    machine_inference_frameworks = []\n",
    "    for key, value in data_dictionary[key_inference_framework].items():\n",
    "        if data_dictionary[key_infrastructure][key] == machine and \\\n",
    "           (value in machine_inference_frameworks) == False:\n",
    "            machine_inference_frameworks.append(value)\n",
    "    inference_frameworks.append(machine_inference_frameworks)\n",
    "\n",
    "# get devices for each inference framework\n",
    "devices = []\n",
    "for idx in range(len(infrastructure)):\n",
    "    machine = infrastructure[idx]\n",
    "    machine_inference_frameworks = inference_frameworks[idx]\n",
    "    machine_framework_devices = []\n",
    "    for inference_framework in machine_inference_frameworks:\n",
    "        framework_devices = []\n",
    "        for key, value in data_dictionary[key_parameters].items():\n",
    "            pattern = re.compile(r'[.]*Device:[ ]*(?P<device_name>[\\W\\w]+)[,]+[.]*')\n",
    "            matcher = re.match(pattern, value)\n",
    "            device_name = matcher.group('device_name')\n",
    "            if data_dictionary[key_infrastructure][key] == machine and \\\n",
    "               data_dictionary[key_inference_framework][key] == inference_framework and \\\n",
    "               (device_name in framework_devices) == False:\n",
    "                framework_devices.append(device_name)\n",
    "        machine_framework_devices.append(framework_devices)\n",
    "    devices.append(machine_framework_devices)\n",
    "\n",
    "print(infrastructure)\n",
    "print(inference_frameworks)\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['FP32'], ['FP32', 'FP16'], ['FP16']]], [[['FP32'], ['FP32', 'FP16'], ['FP16']]], [[['FP32']]]]\n"
     ]
    }
   ],
   "source": [
    "# get precisions for each device\n",
    "precisions = []\n",
    "for idx in range(len(infrastructure)):\n",
    "    machine = infrastructure[idx]\n",
    "    machine_inference_frameworks = inference_frameworks[idx]\n",
    "    machine_precisions = []\n",
    "    for idx2 in range(len(machine_inference_frameworks)):\n",
    "        inference_framework = machine_inference_frameworks[idx2]\n",
    "        framework_devices = devices[idx][idx2]\n",
    "        framework_precisions = []\n",
    "        for device in framework_devices:\n",
    "            device_precisions = []\n",
    "            for key, value in data_dictionary[key_parameters].items():\n",
    "                pattern = re.compile(r'[.]*Device:[ ]*(?P<device_name>[\\W\\w]+)[,]+[.]*')\n",
    "                matcher = re.match(pattern, value)\n",
    "                device_name = matcher.group('device_name')\n",
    "                if data_dictionary[key_infrastructure][key] == machine and \\\n",
    "                   data_dictionary[key_inference_framework][key] == inference_framework and \\\n",
    "                   device_name == device and \\\n",
    "                   (data_dictionary[key_precision][key] in device_precisions) == False:\n",
    "                    device_precisions.append(data_dictionary[key_precision][key])\n",
    "            framework_precisions.append(device_precisions)\n",
    "        machine_precisions.append(framework_precisions)\n",
    "    precisions.append(machine_precisions)           \n",
    "\n",
    "print(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[['Sync', 'Async']], [['Sync', 'Async'], ['Sync', 'Async']], [['Sync', 'Async']]]], [[[['Sync', 'Async']], [['Sync', 'Async'], ['Sync', 'Async']], [['Sync', 'Async']]]], [[[['Sync', 'Async']]]]]\n"
     ]
    }
   ],
   "source": [
    "# get execution modes for each precision\n",
    "execution_modes = []\n",
    "for idx in range(len(infrastructure)):\n",
    "    machine = infrastructure[idx]\n",
    "    machine_inference_frameworks = inference_frameworks[idx]\n",
    "    machine_modes = []\n",
    "    for idx2 in range(len(machine_inference_frameworks)):\n",
    "        inference_framework = machine_inference_frameworks[idx2]\n",
    "        framework_devices = devices[idx][idx2]\n",
    "        framework_modes = []\n",
    "        for idx3 in range(len(framework_devices)):\n",
    "            device = framework_devices[idx3]\n",
    "            device_precisions = precisions[idx][idx2][idx3]\n",
    "            framework_device_modes = []\n",
    "            for precision in device_precisions:\n",
    "                device_precision_modes = []\n",
    "                for key, value in data_dictionary[key_parameters].items():\n",
    "                    pattern = re.compile(r'[.]*Device:[ ]*(?P<device_name>[\\W\\w]+)[,]+[.]*')\n",
    "                    matcher = re.match(pattern, value)\n",
    "                    device_name = matcher.group('device_name')\n",
    "                    if data_dictionary[key_infrastructure][key] == machine and \\\n",
    "                       data_dictionary[key_inference_framework][key] == inference_framework and \\\n",
    "                       device_name == device and \\\n",
    "                       data_dictionary[key_precision][key] == precision and \\\n",
    "                       (data_dictionary[key_execution_mode][key] in device_precision_modes) == False:\n",
    "                        device_precision_modes.append(data_dictionary[key_execution_mode][key])\n",
    "                framework_device_modes.append(device_precision_modes)\n",
    "            framework_modes.append(framework_device_modes)\n",
    "        machine_modes.append(framework_modes)\n",
    "    execution_modes.append(machine_modes)\n",
    "\n",
    "print(execution_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[[5, 6]], [[7, 8], [9, 10]], [[11, 12]]]], [[[[13, 14]], [[15, 16], [17, 18]], [[19, 20]]]], [[[[21, 22]]]]]\n"
     ]
    }
   ],
   "source": [
    "# get column indeces for each execution mode\n",
    "rel_row_idx = 4\n",
    "rel_col_idx = 5\n",
    "\n",
    "col_indeces = []\n",
    "for idx in range(len(infrastructure)):\n",
    "    row_idx = rel_row_idx\n",
    "    col_idx = rel_col_idx\n",
    "    num_cols = 0\n",
    "    machine = infrastructure[idx]\n",
    "    machine_inference_frameworks = inference_frameworks[idx]\n",
    "    col_idx3 = col_idx\n",
    "    col_indeces2 = []\n",
    "    for idx2 in range(len(machine_inference_frameworks)):\n",
    "        machine_framework = machine_inference_frameworks[idx2]\n",
    "        machine_framework_devices = devices[idx][idx2]\n",
    "        col_idx2 = col_idx\n",
    "        num_cols2 = 0\n",
    "        col_indeces3 = []\n",
    "        for idx3 in range(len(machine_framework_devices)):\n",
    "            machine_framework_device = machine_framework_devices[idx3]\n",
    "            framework_device_precisions = precisions[idx][idx2][idx3]\n",
    "            col_idx1 = col_idx\n",
    "            num_cols1 = 0\n",
    "            col_indeces4 = []\n",
    "            for idx4 in range(len(framework_device_precisions)):\n",
    "                framework_device_precision = framework_device_precisions[idx4]\n",
    "                framework_device_precision_modes = execution_modes[idx][idx2][idx3][idx4]\n",
    "                col_indeces5 = []\n",
    "                for idx5 in range(len(framework_device_precision_modes)):\n",
    "                    framework_device_precision_mode = framework_device_precision_modes[idx5]\n",
    "                    sheet.write(row_idx, col_idx + idx5, framework_device_precision_mode, cell_format)\n",
    "                    col_indeces5.append(col_idx + idx5)\n",
    "                sheet.merge_range(row_idx - 1, col_idx1, \\\n",
    "                                  row_idx - 1, col_idx1 + len(framework_device_precision_modes) - 1, \\\n",
    "                                  framework_device_precision, cell_format)\n",
    "                col_idx += len(framework_device_precision_modes)\n",
    "                col_idx1 += len(framework_device_precision_modes)\n",
    "                num_cols1 += len(framework_device_precision_modes)\n",
    "                num_cols2 += len(framework_device_precision_modes)\n",
    "                num_cols += len(framework_device_precision_modes)\n",
    "                col_indeces4.append(col_indeces5)\n",
    "            sheet.merge_range(row_idx - 2, col_idx2, \\\n",
    "                              row_idx - 2, col_idx2 + num_cols1 - 1, \\\n",
    "                              machine_framework_device, cell_format)\n",
    "            col_idx2 += num_cols1\n",
    "            col_indeces3.append(col_indeces4)\n",
    "        sheet.merge_range(row_idx - 3, col_idx3, \\\n",
    "                          row_idx - 3, col_idx3 + num_cols2 - 1, \\\n",
    "                          machine_framework, cell_format)\n",
    "        col_idx3 += num_cols2\n",
    "        col_indeces2.append(col_indeces3)\n",
    "    sheet.merge_range(row_idx - 4, rel_col_idx, \\\n",
    "                      row_idx - 4, rel_col_idx + num_cols - 1, \\\n",
    "                      machine, cell_format)\n",
    "    rel_col_idx += num_cols\n",
    "    col_indeces.append(col_indeces2)\n",
    "\n",
    "print(col_indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full items: 3528\n",
      "Actual items: 3486\n"
     ]
    }
   ],
   "source": [
    "def remove_unused_metrics(data):\n",
    "    # remove unused keys from DataFrame\n",
    "    for key, value in data.copy().items():\n",
    "        if key == key_avgtime or key == key_latency:\n",
    "            del data[key]\n",
    "    return data\n",
    "\n",
    "def remove_failed_experiments(data):\n",
    "    # transpose 2d dictionary\n",
    "    experiments = data.to_dict('index')\n",
    "    print('Full items: {0}'.format(len(experiments)))\n",
    "\n",
    "    # remove failed experiments\n",
    "    for key, value in experiments.copy().items():\n",
    "        if value[key_status] == 'Failed':\n",
    "            del experiments[key]\n",
    "\n",
    "    print('Actual items: {0}'.format(len(experiments)))\n",
    "    return experiments\n",
    "\n",
    "def remove_unused_keys(data):\n",
    "    data = remove_unused_metrics(data)  \n",
    "    experiments = remove_failed_experiments(data)\n",
    "    return experiments\n",
    "\n",
    "experiments = remove_unused_keys(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification: 112\n",
      "semantic segmentation: 6\n",
      "detection: 20\n",
      "object detection: 39\n",
      "object recognition: 11\n",
      "reidentification: 2\n",
      "human pose estimation: 2\n",
      "image processing: 3\n"
     ]
    }
   ],
   "source": [
    "def find_idx(element, available_elements, exception_str):\n",
    "    try:\n",
    "        return available_elements.index(element)\n",
    "    except:\n",
    "        raise exception_str\n",
    "\n",
    "def find_infrastructure_idx(infrastructure_name, available_infrastructure):\n",
    "    return find_idx(infrastructure_name, available_infrastructure, \\\n",
    "                    'Unknown infrastructure')\n",
    "\n",
    "def find_inference_framework_idx(framework_name, available_inference_frameworks):\n",
    "    return find_idx(framework_name, available_inference_frameworks, \\\n",
    "                    'Unknown inference framework')\n",
    "\n",
    "def find_device_idx(device_name, available_devices):\n",
    "    return find_idx(device_name, available_devices, 'Unknown device name')\n",
    "\n",
    "def find_precision_idx(precision, available_precisions):\n",
    "    return find_idx(precision, available_precisions, 'Unknown precision')\n",
    "\n",
    "def find_execution_mode_idx(execution_mode, available_execution_modes):\n",
    "    return find_idx(execution_mode, available_execution_modes, \\\n",
    "                    'Unknown execution mode')\n",
    "\n",
    "def find_column_idx(value, infrastructure, inference_frameworks, devices, \\\n",
    "                    precisions, execution_modes, col_indeces):\n",
    "    idx1 = find_infrastructure_idx(value[key_infrastructure], infrastructure)    \n",
    "    idx2 = find_inference_framework_idx(value[key_inference_framework], inference_frameworks[idx1])\n",
    "    pattern = re.compile(r'[.]*Device:[ ]*(?P<device_name>[\\W\\w]+)[,]+[.]*')\n",
    "    matcher = re.match(pattern, value[key_parameters])\n",
    "    device_name = matcher.group('device_name')\n",
    "    idx3 = find_device_idx(device_name, devices[idx1][idx2])\n",
    "    idx4 = find_precision_idx(value[key_precision], precisions[idx1][idx2][idx3])\n",
    "    idx5 = find_execution_mode_idx(value[key_execution_mode], execution_modes[idx1][idx2][idx3][idx4])\n",
    "    return col_indeces[idx1][idx2][idx3][idx4][idx5]\n",
    "\n",
    "def find_row_records(task_type, topology_name, train_framework, \\\n",
    "                     blob_size, batch_size, experiments, processed_records_keys):\n",
    "    records_group = []\n",
    "    for key, value in experiments.items():\n",
    "        if key not in processed_records_keys and \\\n",
    "           value[key_task_type] == task_type and \\\n",
    "           value[key_topology_name] == topology_name and \\\n",
    "           value[key_train_framework] == train_framework and \\\n",
    "           value[key_blob_size] == blob_size and \\\n",
    "           value[key_batch_size] == batch_size:\n",
    "            records_group.append(value)\n",
    "            processed_records_keys.append(key)\n",
    "    return records_group\n",
    "\n",
    "def create_row_record(records_group, infrastructure, inference_frameworks, devices, \\\n",
    "                      precisions, execution_modes, col_indeces):\n",
    "    fps_record = {}\n",
    "    for value in records_group:\n",
    "        col_idx = find_column_idx(value, infrastructure, inference_frameworks, \\\n",
    "            devices, precisions, execution_modes, col_indeces)\n",
    "        fps_record.update( {col_idx : value[key_fps]} )\n",
    "    return fps_record\n",
    "\n",
    "\n",
    "processed_records_keys = []\n",
    "table_records = defaultdict(list)\n",
    "for key, value in experiments.items():\n",
    "    if key in processed_records_keys:\n",
    "        continue\n",
    "    records_group = find_row_records(value[key_task_type], value[key_topology_name], \\\n",
    "        value[key_train_framework], value[key_blob_size], value[key_batch_size], \\\n",
    "        experiments, processed_records_keys)\n",
    "    fps_record = create_row_record(records_group, infrastructure, inference_frameworks, \\\n",
    "        devices, precisions, execution_modes, col_indeces)\n",
    "    record = { key_task_type : value[key_task_type], key_topology_name : value[key_topology_name], \\\n",
    "               key_train_framework : value[key_train_framework], key_blob_size : value[key_blob_size], \\\n",
    "               key_batch_size : value[key_batch_size], key_fps : fps_record }\n",
    "    table_records[value[key_task_type]].append(record)\n",
    "\n",
    "for key, value in table_records.items():\n",
    "    print('{}: {}'.format(key, len(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification: 112\n",
      "alexnet, 2\n",
      "alexnet, 0\n",
      "caffenet, 2\n",
      "caffenet, 0\n",
      "densenet-121, 2\n",
      "densenet-121, 0\n",
      "densenet-161, 2\n",
      "densenet-161, 0\n",
      "densenet-169, 2\n",
      "densenet-169, 0\n",
      "densenet-201, 2\n",
      "densenet-201, 0\n",
      "densenet-121-tf, 2\n",
      "densenet-121-tf, 0\n",
      "densenet-169-tf, 2\n",
      "densenet-169-tf, 0\n",
      "efficientnet-b0, 2\n",
      "efficientnet-b0, 0\n",
      "efficientnet-b0_auto_aug, 2\n",
      "efficientnet-b0_auto_aug, 0\n",
      "efficientnet-b0-pytorch, 2\n",
      "efficientnet-b0-pytorch, 0\n",
      "efficientnet-b5, 2\n",
      "efficientnet-b5, 0\n",
      "efficientnet-b5-pytorch, 2\n",
      "efficientnet-b5-pytorch, 0\n",
      "efficientnet-b7-pytorch, 2\n",
      "efficientnet-b7-pytorch, 0\n",
      "efficientnet-b7_auto_aug, 2\n",
      "efficientnet-b7_auto_aug, 0\n",
      "googlenet-v1, 2\n",
      "googlenet-v1, 0\n",
      "googlenet-v2, 2\n",
      "googlenet-v2, 0\n",
      "googlenet-v3, 2\n",
      "googlenet-v3, 0\n",
      "googlenet-v3-pytorch, 2\n",
      "googlenet-v3-pytorch, 0\n",
      "googlenet-v4-tf, 2\n",
      "googlenet-v4-tf, 0\n",
      "googlenet-v1-tf, 2\n",
      "googlenet-v1-tf, 0\n",
      "inception-resnet-v2-tf, 2\n",
      "inception-resnet-v2-tf, 0\n",
      "mobilenet-v1-0.25-128, 2\n",
      "mobilenet-v1-0.25-128, 0\n",
      "mobilenet-v1-0.50-160, 2\n",
      "mobilenet-v1-0.50-160, 0\n",
      "mobilenet-v1-0.50-224, 2\n",
      "mobilenet-v1-0.50-224, 0\n",
      "mobilenet-v1-1.0-224, 2\n",
      "mobilenet-v1-1.0-224, 0\n",
      "mobilenet-v1-1.0-224-tf, 2\n",
      "mobilenet-v1-1.0-224-tf, 0\n",
      "mobilenet-v2, 2\n",
      "mobilenet-v2, 0\n",
      "mobilenet-v2-pytorch, 2\n",
      "mobilenet-v2-pytorch, 0\n",
      "mobilenet-v2-1.4-224, 2\n",
      "mobilenet-v2-1.4-224, 0\n",
      "mobilenet-v2-1.0-224, 2\n",
      "mobilenet-v2-1.0-224, 0\n",
      "resnet-50-tf, 2\n",
      "resnet-50-tf, 0\n",
      "resnet-50-pytorch, 2\n",
      "resnet-50-pytorch, 0\n",
      "resnet-18-pytorch, 2\n",
      "resnet-18-pytorch, 0\n",
      "se-inception, 2\n",
      "se-inception, 0\n",
      "se-resnet-101, 2\n",
      "se-resnet-101, 0\n",
      "se-resnet-152, 2\n",
      "se-resnet-152, 0\n",
      "squeezenet1.0, 2\n",
      "squeezenet1.0, 0\n",
      "squeezenet1.1, 2\n",
      "squeezenet1.1, 0\n",
      "vgg16, 2\n",
      "vgg16, 0\n",
      "vgg19, 2\n",
      "vgg19, 0\n",
      "squeezenet1.1-caffe2, 2\n",
      "squeezenet1.1-caffe2, 0\n",
      "resnet-50-caffe2, 2\n",
      "resnet-50-caffe2, 0\n",
      "vgg19-caffe2, 2\n",
      "vgg19-caffe2, 0\n",
      "densenet-121-caffe2, 2\n",
      "densenet-121-caffe2, 0\n",
      "googlenet-v2-tf, 2\n",
      "googlenet-v2-tf, 0\n",
      "mobilenet-v3-large-1.0-224-tf, 2\n",
      "mobilenet-v3-large-1.0-224-tf, 0\n",
      "mobilenet-v3-small-1.0-224-tf, 2\n",
      "mobilenet-v3-small-1.0-224-tf, 0\n",
      "octave-densenet-121-0.125, 2\n",
      "octave-densenet-121-0.125, 0\n",
      "octave-resnet-26-0.25, 2\n",
      "octave-resnet-26-0.25, 0\n",
      "octave-resnet-50-0.125, 2\n",
      "octave-resnet-50-0.125, 0\n",
      "octave-resnet-101-0.125, 2\n",
      "octave-resnet-101-0.125, 0\n",
      "octave-resnet-200-0.125, 2\n",
      "octave-resnet-200-0.125, 0\n",
      "octave-resnext-50-0.25, 2\n",
      "octave-resnext-50-0.25, 0\n",
      "octave-resnext-101-0.25, 2\n",
      "octave-resnext-101-0.25, 0\n",
      "octave-se-resnet-50-0.125, 2\n",
      "octave-se-resnet-50-0.125, 0\n",
      "semantic segmentation: 6\n",
      "deeplabv3, 2\n",
      "deeplabv3, 0\n",
      "road-segmentation-adas-0001, 2\n",
      "road-segmentation-adas-0001, 0\n",
      "semantic-segmentation-adas-0001, 2\n",
      "semantic-segmentation-adas-0001, 0\n",
      "detection: 20\n",
      "ssd300, 2\n",
      "ssd300, 0\n",
      "ssd512, 2\n",
      "ssd512, 0\n",
      "ssd_mobilenet_v1_coco, 2\n",
      "ssd_mobilenet_v1_coco, 0\n",
      "mobilenet-ssd, 2\n",
      "mobilenet-ssd, 0\n",
      "ssd_mobilenet_v1_fpn_coco, 2\n",
      "ssd_mobilenet_v1_fpn_coco, 0\n",
      "ssd_mobilenet_v2_coco, 2\n",
      "ssd_mobilenet_v2_coco, 0\n",
      "yolo-v1-tiny-tf, 2\n",
      "yolo-v1-tiny-tf, 0\n",
      "yolo-v2-tiny-tf, 2\n",
      "yolo-v2-tiny-tf, 0\n",
      "yolo-v2-tf, 2\n",
      "yolo-v2-tf, 0\n",
      "ssd_resnet50_v1_fpn_coco, 2\n",
      "ssd_resnet50_v1_fpn_coco, 0\n",
      "object detection: 39\n",
      "face-detection-adas-0001, 2\n",
      "face-detection-adas-0001, 0\n",
      "face-detection-retail-0004, 2\n",
      "face-detection-retail-0004, 0\n",
      "face-detection-retail-0005, 2\n",
      "face-detection-retail-0005, 0\n",
      "person-detection-retail-0002, 2\n",
      "person-detection-retail-0002, 0\n",
      "person-detection-retail-0013, 2\n",
      "person-detection-retail-0013, 0\n",
      "person-detection-action-recognition-0005, 2\n",
      "person-detection-action-recognition-0005, 0\n",
      "person-detection-action-recognition-0006, 2\n",
      "person-detection-action-recognition-0006, 0\n",
      "person-detection-action-recognition-teacher-0002, 2\n",
      "person-detection-action-recognition-teacher-0002, 0\n",
      "pedestrian-detection-adas-0002, 2\n",
      "pedestrian-detection-adas-0002, 0\n",
      "pedestrian-and-vehicle-detector-adas-0001, 2\n",
      "pedestrian-and-vehicle-detector-adas-0001, 0\n",
      "person-vehicle-bike-detection-crossroad-0078, 2\n",
      "person-vehicle-bike-detection-crossroad-0078, 0\n",
      "person-vehicle-bike-detection-crossroad-1016, 2\n",
      "person-vehicle-bike-detection-crossroad-1016, 0\n",
      "vehicle-detection-adas-0002, 1\n",
      "product-detection-0001, 2\n",
      "product-detection-0001, 0\n",
      "yolo-v2-ava-0001, 2\n",
      "yolo-v2-ava-0001, 0\n",
      "yolo-v2-ava-sparse-35-0001, 2\n",
      "yolo-v2-ava-sparse-35-0001, 0\n",
      "yolo-v2-ava-sparse-70-0001, 2\n",
      "yolo-v2-ava-sparse-70-0001, 0\n",
      "yolo-v2-tiny-ava-0001, 2\n",
      "yolo-v2-tiny-ava-0001, 0\n",
      "yolo-v2-tiny-ava-sparse-30-0001, 2\n",
      "yolo-v2-tiny-ava-sparse-30-0001, 0\n",
      "yolo-v2-tiny-ava-sparse-60-0001, 2\n",
      "yolo-v2-tiny-ava-sparse-60-0001, 0\n",
      "object recognition: 11\n",
      "age-gender-recognition-retail-0013, 2\n",
      "age-gender-recognition-retail-0013, 0\n",
      "head-pose-estimation-adas-0001, 2\n",
      "head-pose-estimation-adas-0001, 0\n",
      "landmarks-regression-retail-0009, 2\n",
      "landmarks-regression-retail-0009, 0\n",
      "facial-landmarks-35-adas-0002, 2\n",
      "facial-landmarks-35-adas-0002, 0\n",
      "person-attributes-recognition-crossroad-0230, 2\n",
      "person-attributes-recognition-crossroad-0230, 0\n",
      "gaze-estimation-adas-0002, 1\n",
      "reidentification: 2\n",
      "face-reidentification-retail-0095, 2\n",
      "face-reidentification-retail-0095, 0\n",
      "human pose estimation: 2\n",
      "human-pose-estimation-0001, 2\n",
      "human-pose-estimation-0001, 0\n",
      "image processing: 3\n",
      "single-image-super-resolution-1032, 1\n",
      "single-image-super-resolution-1033, 2\n",
      "single-image-super-resolution-1033, 0\n"
     ]
    }
   ],
   "source": [
    "def get_records_group(task_records, topology_name, train_framework, \\\n",
    "                      blob_size, processed_records_idxs):\n",
    "    records_group = []\n",
    "    for idx in range(len(task_records)):\n",
    "        record = task_records[idx]\n",
    "        if idx not in processed_records_idxs and \\\n",
    "           record[key_topology_name] == topology_name and \\\n",
    "           record[key_train_framework] == train_framework and \\\n",
    "           record[key_blob_size] == blob_size:\n",
    "            processed_records_idxs.append(idx)\n",
    "            records_group.append(record)\n",
    "    return records_group\n",
    "\n",
    "def save_table_rows(table_records, sheet):\n",
    "    row_idx = 5\n",
    "    for task_type, task_records in table_records.items(): # loop by tasks\n",
    "        print('{}: {}'.format(task_type, len(task_records)))\n",
    "        if len(task_records) <= 0:\n",
    "            continue\n",
    "        elif len(task_records) > 1: # print task type\n",
    "            sheet.merge_range(row_idx, 0, row_idx + len(task_records) - 1, 0, task_type)\n",
    "        else:\n",
    "            sheet.write(row_idx, 0, task_type)\n",
    "        \n",
    "        processed_records_idxs = []\n",
    "        for record in task_records: # loop by table records and searching for records for the same topologies\n",
    "            topology_name = record[key_topology_name]\n",
    "            train_framework = record[key_train_framework]\n",
    "            blob_size = record[key_blob_size]\n",
    "            records_group = get_records_group(task_records, topology_name, \\\n",
    "                                              train_framework, blob_size, \\\n",
    "                                              processed_records_idxs)\n",
    "            topology_num_records = len(records_group)\n",
    "            print('{}, {}'.format(topology_name, topology_num_records))\n",
    "            if topology_num_records == 0:\n",
    "                continue\n",
    "            elif topology_num_records > 1:\n",
    "                sheet.merge_range(row_idx, 1, row_idx + topology_num_records - 1, 1, topology_name)\n",
    "                sheet.merge_range(row_idx, 2, row_idx + topology_num_records - 1, 2, train_framework)\n",
    "                sheet.merge_range(row_idx, 3, row_idx + topology_num_records - 1, 3, blob_size)\n",
    "            else:\n",
    "                sheet.write(row_idx, 1, topology_name)\n",
    "                sheet.write(row_idx, 2, train_framework)\n",
    "                sheet.write(row_idx, 3, blob_size)\n",
    "            for topology_record in records_group:\n",
    "                sheet.write(row_idx, 4, topology_record[key_batch_size])\n",
    "                for key, value in topology_record[key_fps].items():\n",
    "                    sheet.write(row_idx, key, float(value))\n",
    "                row_idx += 1\n",
    "    \n",
    "        \n",
    "save_table_rows(table_records, sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
